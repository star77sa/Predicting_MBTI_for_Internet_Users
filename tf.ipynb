{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a214dfa6-937f-4306-bf82-911785775ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "\n",
    "import abc\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50d15a00-3a86-45be-ad76-27dfc546865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c5a1a6-7e44-4c9f-9620-f84bb5cdb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐글 데이터\n",
    "df = pd.read_csv('G://내 드라이브/Github/Predictiong_MBTI_for_Internet_Users/MBTI 500.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21681be3-3c63-480b-a58e-d1ac8df11067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text', 'type']\n",
    "df = df[['type', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b318a78b-4193-4862-ab37-3c69d37a7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 106067/106067 [00:20<00:00, 5217.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#function to clean the text data\n",
    "def clear_text(data):\n",
    "    data_length=[]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    cleaned_text=[]\n",
    "    for sentence in tqdm(data.text):\n",
    "        sentence=sentence.lower()\n",
    "#         removing links from text data\n",
    "        sentence=re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',sentence)\n",
    "#         removing other symbols\n",
    "        sentence=re.sub('[^0-9a-z]',' ',sentence)\n",
    "        data_length.append(len(sentence.split()))\n",
    "        cleaned_text.append(sentence)\n",
    "    return cleaned_text,data_length\n",
    "\n",
    "df.text, _=clear_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37784595-92c4-4b13-9831-32a18cfc47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'] # features\n",
    "y = df['type']  # labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571aa18a-1488-4409-bdb8-2ac94b64ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelining the vectorizer and the classifier\n",
    "text_clf1 = Pipeline([('tfidf',TfidfVectorizer(sublinear_tf = True)),('clf',LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed469971-ae74-4491-85b2-37899a067135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(sublinear_tf=True)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c9f14f-2fa1-4ee6-a628-5bb62f02a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = text_clf1.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478facc-8eb3-4f1a-8420-6f91e9563105",
   "metadata": {},
   "source": [
    "+++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3174eaad-3f4e-4295-b36f-93f5add7054b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4975d37-07be-4981-bfe4-f123a91075aa",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "889ea8e8-f6b1-46f4-adfd-9abb1ec9dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 3 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 3  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 3  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 1# @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3edd80c1-d8f6-4a5a-a139-8f01a80889dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dcbccc5a-f305-4efc-963b-ba17123dda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()\n",
    "\n",
    "class LinegameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=100, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 1.0\n",
    "        self._time = 1\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 1.0\n",
    "        self._time = 1\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            self.reset()\n",
    "        elif self._time==1: \n",
    "            print(\"### new episode start! ###\")\n",
    "            print(\" \")\n",
    "        else: \n",
    "            print(\" \")\n",
    "        \n",
    "        print(\"{}-1. current state = {}\".format(self._time,self._state))\n",
    "        \n",
    "        # Pipelining the vectorizer and the classifier\n",
    "        text_clf1 = Pipeline([('tfidf',TfidfVectorizer(sublinear_tf = True)),('clf',LinearSVC(C= self._state))])\n",
    "        text_clf1.fit(X_train, y_train)\n",
    "        \n",
    "        reward = metrics.accuracy_score(y_val, text_clf1.predict(X_val))\n",
    "        \n",
    "        #self._state += (action-500)/500\n",
    "        #print(\"{}-2. action = {}\".format(self._time,(action-500)/500))\n",
    "        self._state += (action-50)/500\n",
    "        print(\"{}-2. action = {}\".format(self._time,(action-50)/500))\n",
    "        \n",
    "        \n",
    "        if self._state < 0.1:\n",
    "          print(\"    **exit(current state<0.1) => reset current state as 0.**\")\n",
    "          self._state = 0.1\n",
    "        if self._state > 1.0 : \n",
    "          print(\"   **exit(current state >1.0) => reset current state as 1.**\")\n",
    "          self._state = 1.0\n",
    "        \n",
    "        print(\"{}-3. reward = {}\".format(self._time,reward))\n",
    "        returns.append(reward) ############################# !!!\n",
    "        \n",
    "        print(\"{}-4. future state = {}\".format(self._time,self._state))\n",
    "        \n",
    "        self._time += 1\n",
    "        if self._time > 100: self._episode_ended = True\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            print(\" \")\n",
    "            print(\"### episode ended ###\")\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97396a5b-a99a-4c65-a426-62155444a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### new episode start! ###\n",
      " \n",
      "1-1. current state = 1.0\n",
      "1-2. action = -0.024\n",
      "1-3. reward = 0.8440652399358914\n",
      "1-4. future state = 0.976\n",
      " \n",
      "2-1. current state = 0.976\n",
      "2-2. action = -0.096\n",
      "2-3. reward = 0.8440652399358914\n",
      "2-4. future state = 0.88\n",
      " \n",
      "3-1. current state = 0.88\n",
      "3-2. action = -0.032\n",
      "3-3. reward = 0.8440652399358914\n",
      "3-4. future state = 0.848\n",
      " \n",
      "4-1. current state = 0.848\n",
      "4-2. action = -0.018\n",
      "4-3. reward = 0.844442349391911\n",
      "4-4. future state = 0.83\n",
      " \n",
      "5-1. current state = 0.83\n",
      "5-2. action = -0.076\n",
      "5-3. reward = 0.8448194588479306\n",
      "5-4. future state = 0.754\n",
      " \n",
      "6-1. current state = 0.754\n",
      "6-2. action = -0.078\n",
      "6-3. reward = 0.8451965683039502\n",
      "6-4. future state = 0.676\n",
      " \n",
      "7-1. current state = 0.676\n",
      "7-2. action = 0.028\n",
      "7-3. reward = 0.8460450645799943\n",
      "7-4. future state = 0.7040000000000001\n",
      " \n",
      "8-1. current state = 0.7040000000000001\n",
      "8-2. action = -0.046\n",
      "8-3. reward = 0.8459507872159895\n",
      "8-4. future state = 0.658\n",
      " \n",
      "9-1. current state = 0.658\n",
      "9-2. action = 0.014\n",
      "9-3. reward = 0.8466107287640238\n",
      "9-4. future state = 0.672\n",
      " \n",
      "10-1. current state = 0.672\n",
      "10-2. action = -0.034\n",
      "10-3. reward = 0.8462336193080041\n",
      "10-4. future state = 0.638\n",
      " \n",
      "11-1. current state = 0.638\n",
      "11-2. action = 0.07\n",
      "11-3. reward = 0.846327896672009\n",
      "11-4. future state = 0.708\n",
      " \n",
      "12-1. current state = 0.708\n",
      "12-2. action = 0.014\n",
      "12-3. reward = 0.8459507872159895\n",
      "12-4. future state = 0.722\n",
      " \n",
      "13-1. current state = 0.722\n",
      "13-2. action = 0.06\n",
      "13-3. reward = 0.8457622324879797\n",
      "13-4. future state = 0.782\n",
      " \n",
      "14-1. current state = 0.782\n",
      "14-2. action = -0.05\n",
      "14-3. reward = 0.8452908456679551\n",
      "14-4. future state = 0.732\n",
      " \n",
      "15-1. current state = 0.732\n",
      "15-2. action = 0.022\n",
      "15-3. reward = 0.8456679551239747\n",
      "15-4. future state = 0.754\n",
      " \n",
      "16-1. current state = 0.754\n",
      "16-2. action = 0.01\n",
      "16-3. reward = 0.8451965683039502\n",
      "16-4. future state = 0.764\n",
      " \n",
      "17-1. current state = 0.764\n",
      "17-2. action = -0.056\n",
      "17-3. reward = 0.8451965683039502\n",
      "17-4. future state = 0.708\n",
      " \n",
      "18-1. current state = 0.708\n",
      "18-2. action = -0.01\n",
      "18-3. reward = 0.8459507872159895\n",
      "18-4. future state = 0.698\n",
      " \n",
      "19-1. current state = 0.698\n",
      "19-2. action = 0.052\n",
      "19-3. reward = 0.8459507872159895\n",
      "19-4. future state = 0.75\n",
      " \n",
      "20-1. current state = 0.75\n",
      "20-2. action = -0.076\n",
      "20-3. reward = 0.8451965683039502\n",
      "20-4. future state = 0.674\n",
      " \n",
      "21-1. current state = 0.674\n",
      "21-2. action = -0.074\n",
      "21-3. reward = 0.8461393419439992\n",
      "21-4. future state = 0.6000000000000001\n",
      " \n",
      "22-1. current state = 0.6000000000000001\n",
      "22-2. action = 0.08\n",
      "22-3. reward = 0.8460450645799943\n",
      "22-4. future state = 0.68\n",
      " \n",
      "23-1. current state = 0.68\n",
      "23-2. action = 0.056\n",
      "23-3. reward = 0.8460450645799943\n",
      "23-4. future state = 0.7360000000000001\n",
      " \n",
      "24-1. current state = 0.7360000000000001\n",
      "24-2. action = 0.024\n",
      "24-3. reward = 0.8457622324879797\n",
      "24-4. future state = 0.7600000000000001\n",
      " \n",
      "25-1. current state = 0.7600000000000001\n",
      "25-2. action = -0.056\n",
      "25-3. reward = 0.8451022909399453\n",
      "25-4. future state = 0.7040000000000001\n",
      " \n",
      "26-1. current state = 0.7040000000000001\n"
     ]
    }
   ],
   "source": [
    "environment = LinegameEnv()\n",
    "utils.validate_py_environment(environment, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "30db82bc-9f86-4477-b22c-f8fca0e0004f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8407655321957198,\n",
       " 0.847364947676063,\n",
       " 0.8414254737437541,\n",
       " 0.8407655321957198,\n",
       " 0.8468935608560385,\n",
       " 0.848307721316112,\n",
       " 0.8470821155840482,\n",
       " 0.8464221740360139,\n",
       " 0.8446309041199208,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8426510794758179,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8481191665881022,\n",
       " 0.8420854152917885,\n",
       " 0.8474592250400679,\n",
       " 0.8495333270481757]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9abe44bf-55d8-4d35-80cf-19b30f31d1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#iterations = range(0, 2*7 + 1, 1)\n",
    "iterations = range(0,100)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "29481664-2f3c-4dc7-ba62-0406743ca5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = LinegameEnv\n",
    "eval_py_env = LinegameEnv\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe0b61-a405-4327-a9b5-a77d770e26fc",
   "metadata": {},
   "source": [
    "### 에이전트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c6f70fba-e29f-4f39-b920-b3cb4f62602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step_counter = tf.Variable(0)\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd3c2c-ff20-4fec-b48f-b6bd02efbd71",
   "metadata": {},
   "source": [
    "### Policies\n",
    "\n",
    "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
    "\n",
    "In this tutorial:\n",
    "\n",
    "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
    "-   The policy returns an action (left or right) for each `time_step` observation.\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6e90c60e-0d1e-41b2-b0c6-2a45033e2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed967874-eb82-4041-a137-46170d890790",
   "metadata": {},
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a679b831-73c5-4965-b435-449d05e7104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669e6ca-e991-436e-85fb-c8b5f5697de7",
   "metadata": {},
   "source": [
    "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439c44c-e78c-46f5-914d-d1be049cef21",
   "metadata": {},
   "source": [
    "### Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n",
    "\n",
    "정책을 평가하는 데 사용되는 가장 일반적인 메트릭은 평균 수익률입니다. 수익률은 한 에피소드에 대한 환경에서 정책을 실행하는 동안 얻은 보상의 합계입니다. 몇 개의 에피소드가 실행되어 평균 수익을 창출합니다. 다음 함수는 정책, 환경 및 에피소드 수에 따라 정책의 평균 수익률을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3dbcc350-de50-4d9b-a41b-6f1f35d053d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "953834ae-d8ab-46a8-84c9-29fe7cdea7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fcb6ac89-a4e6-45b9-b7d1-982cb4e0eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            print(f\"{time_step.reward} 현재의 리워드다다다다다\")\n",
    "        total_return += episode_return\n",
    "        print(f\"{episode_return}이다다다ㅏ다다\")\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0d5ce-9d0b-45fb-9069-8b65b0a04d4d",
   "metadata": {},
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "894a8815-c7c6-4519-9a6a-b99784026327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### new episode start! ###\n",
      " \n",
      "1-1. current state = 0.1\n",
      "1-2. action = 0.09\n",
      "1-3. reward = 0.8407655321957198\n",
      "1-4. future state = 0.19\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "2-1. current state = 0.19\n",
      "2-2. action = 0.092\n",
      "2-3. reward = 0.848307721316112\n",
      "2-4. future state = 0.28200000000000003\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "3-1. current state = 0.28200000000000003\n",
      "3-2. action = -0.094\n",
      "3-3. reward = 0.8494390496841708\n",
      "3-4. future state = 0.18800000000000003\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "4-1. current state = 0.18800000000000003\n",
      "4-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "4-3. reward = 0.848307721316112\n",
      "4-4. future state = 0.1\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "5-1. current state = 0.1\n",
      "5-2. action = 0.038\n",
      "5-3. reward = 0.8407655321957198\n",
      "5-4. future state = 0.138\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "6-1. current state = 0.138\n",
      "6-2. action = -0.074\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "6-3. reward = 0.84538512303196\n",
      "6-4. future state = 0.1\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "7-1. current state = 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-3b7f8579967a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_avg_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-153-74d4291e3b08>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[1;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mepisode_return\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{time_step.reward} 현재의 리워드다다다다다\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;34m'but saw action with shape %s:\\n   %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 (self.batch_size, action.shape, action))\n\u001b[1;32m--> 315\u001b[1;33m       outputs = tf.numpy_function(\n\u001b[0m\u001b[0;32m    316\u001b[0m           \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m           \u001b[0mflat_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mnumpy_function\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[0mSingle\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcomputes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m   \"\"\"\n\u001b[1;32m--> 770\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mpy_func_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mpy_func_common\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    634\u001b[0m   \"\"\"\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_isolated_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_step_py\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    296\u001b[0m         packed = tf.nest.pack_sequence_as(\n\u001b[0;32m    297\u001b[0m             structure=self.action_spec(), flat_sequence=flattened_actions)\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\batched_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_envs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m       \u001b[0mtime_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-fdda9641a9fe>\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Pipelining the vectorizer and the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtext_clf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublinear_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0m\u001b[0;32m    976\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19265a-86e6-4792-a73b-8530626e1dff",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n",
    "\n",
    "재생 버퍼는 환경에서 수집된 데이터를 추적합니다. 이 자습서에서는 tf_agents.replay_buffers.tf_uniform_replay_buffer를 사용합니다.가장 일반적인 TFUniformReplayBuffer입니다. 생성자는 수집할 데이터에 대한 사양을 요구합니다. 이 기능은 collect_data_spec 메서드를 사용하여 에이전트에서 사용할 수 있습니다. 배치 크기와 최대 버퍼 길이도 필요합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d288b12-690d-4efa-9e69-bcb5226521a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd3640-3827-4b78-9cd0-14e4fc92c372",
   "metadata": {},
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab33d472-3a88-4402-9158-6e2b363c7751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(100)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array(0), maximum=array(2147483647)),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b328aeab-9cc8-43ac-afda-dc7b0eb3caff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fab0a-1d89-4d2a-884e-8086b4e1f025",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a8340a15-7519-4b39-83c2-779556cc602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "4-1. current state = 0.1\n",
      "4-2. action = -0.058\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "4-3. reward = 0.8407655321957198\n",
      "4-4. future state = 0.1\n"
     ]
    }
   ],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=1)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcfb50-6e21-4e2b-a1fa-8c92adebdbab",
   "metadata": {},
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9582980c-f8b1-4adc-8151-e1854bf9ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the curious:\n",
    "# Uncomment to peel one of these off and inspect it.\n",
    "# iter(replay_buffer.as_dataset()).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b88f79-45a4-4620-9b84-5e3110562a05",
   "metadata": {},
   "source": [
    "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "This dataset is also optimized by running parallel calls and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "29a6ba62-edaa-4414-a08f-ec7aef72e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(Trajectory(\n",
       "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(64, 2, 1), dtype=tf.int32, name=None),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), BufferInfo(ids=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), probabilities=TensorSpec(shape=(64,), dtype=tf.float32, name=None)))>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "817fe5f9-5f0f-4009-a854-2cf12987ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026D2B103340>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "72e14f16-d4b6-4ad5-96c7-fcbb1b96374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the curious:\n",
    "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
    "# Compare this representation of replay data \n",
    "# to the collection of individual trajectories shown earlier.\n",
    "\n",
    "# iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3f375-becf-4212-82ea-367dc8085c02",
   "metadata": {},
   "source": [
    "### Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa52b1d4-c6ef-4192-946d-b4b45cf34028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### new episode start! ###\n",
      " \n",
      "1-1. current state = 0.1\n",
      "1-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "1-3. reward = 0.8407655321957198\n",
      "1-4. future state = 0.1\n",
      " \n",
      "2-1. current state = 0.1\n",
      "2-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "2-3. reward = 0.8407655321957198\n",
      "2-4. future state = 0.1\n",
      " \n",
      "3-1. current state = 0.1\n",
      "3-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "3-3. reward = 0.8407655321957198\n",
      "3-4. future state = 0.1\n",
      " \n",
      "4-1. current state = 0.1\n",
      "4-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "4-3. reward = 0.8407655321957198\n",
      "4-4. future state = 0.1\n",
      " \n",
      "5-1. current state = 0.1\n",
      "5-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "5-3. reward = 0.8407655321957198\n",
      "5-4. future state = 0.1\n",
      " \n",
      "6-1. current state = 0.1\n",
      "6-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "6-3. reward = 0.8407655321957198\n",
      "6-4. future state = 0.1\n",
      " \n",
      "7-1. current state = 0.1\n",
      "7-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "7-3. reward = 0.8407655321957198\n",
      "7-4. future state = 0.1\n",
      " \n",
      "### episode ended ###\n",
      " \n",
      "5-1. current state = 0.1\n",
      "5-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "5-3. reward = 0.8407655321957198\n",
      "5-4. future state = 0.1\n",
      " \n",
      "6-1. current state = 0.1\n",
      "6-2. action = -0.026\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "6-3. reward = 0.8407655321957198\n",
      "6-4. future state = 0.1\n",
      " \n",
      "7-1. current state = 0.1\n",
      "7-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "7-3. reward = 0.8407655321957198\n",
      "7-4. future state = 0.1\n",
      " \n",
      "### episode ended ###\n",
      "WARNING:tensorflow:5 out of the last 28 calls to <tensorflow.python.eager.def_function.Function object at 0x0000026D3A4F45E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 28 calls to <tensorflow.python.eager.def_function.Function object at 0x0000026D3A4F45E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1. current state = 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-462ee3ad05c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollect_steps_per_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mcollect_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-144-64fcfe51e8ac>\u001b[0m in \u001b[0;36mcollect_step\u001b[1;34m(environment, policy, buffer)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnext_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtraj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;34m'but saw action with shape %s:\\n   %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 (self.batch_size, action.shape, action))\n\u001b[1;32m--> 315\u001b[1;33m       outputs = tf.numpy_function(\n\u001b[0m\u001b[0;32m    316\u001b[0m           \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m           \u001b[0mflat_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mnumpy_function\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[0mSingle\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcomputes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m   \"\"\"\n\u001b[1;32m--> 770\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mpy_func_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mpy_func_common\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    634\u001b[0m   \"\"\"\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_isolated_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_step_py\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    296\u001b[0m         packed = tf.nest.pack_sequence_as(\n\u001b[0;32m    297\u001b[0m             structure=self.action_spec(), flat_sequence=flattened_actions)\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\batched_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_envs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m       \u001b[0mtime_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-fdda9641a9fe>\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Pipelining the vectorizer and the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtext_clf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublinear_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                     \u001b[1;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96669f69-ca6e-4528-a274-3e6fe36aa939",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a3f1f-5c8d-4e34-ac1b-778e3d40cadb",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
    "\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ad1d7b56-4462-4a6f-808e-e4c60664b92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVlklEQVR4nO3df/BddX3n8eeLYIr8FEuKQrIbtBSI7oLOV1bBOhQc+VGE1nUqqN3d2CmbXahI60i6trvusDtjVbp2G7aYUkRbFLuAXXBc0HYVt+6ofAOEmCDbNCikiITSCkt3jYH3/nHP116++STfm+R7vveb5PmYufO955zPOff94c7kxTmfe84nVYUkSdMdMO4CJEnzkwEhSWoyICRJTQaEJKnJgJAkNR047gJm01FHHVVLly4ddxmStNdYs2bNE1W1qLVtnwqIpUuXMjk5Oe4yJGmvkeQ7O9rmJSZJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqanXgEhyTpIHk2xMsrKx/YgktydZm2R9kuVD267o1n0zyaeTHNRnrZKk5+stIJIsAK4BzgWWARcnWTat2aXAhqo6GTgDuDrJwiTHAu8GJqrqlcAC4KK+apUkba/PM4hTgY1VtamqtgI3ARdOa1PAYUkCHAo8CWzrth0IvDDJgcDBwKM91ipJmqbPgDgWeGRoeXO3btgq4CQG//ivAy6vqueq6q+AjwAPA98Fvl9VX2h9SJJLkkwmmdyyZcts90GS9lt9BkQa62ra8tnAfcAxwCnAqiSHJzmSwdnGcd22Q5K8s/UhVbW6qiaqamLRouakSJKk3dBnQGwGlgwtL2b7y0TLgVtrYCPwEHAi8EbgoaraUlU/BG4FTuuxVknSNH0GxN3A8UmOS7KQwSDzbdPaPAycBZDkaOAEYFO3/rVJDu7GJ84CHuixVknSNL3NSV1V25JcBtzJ4FdI11fV+iQruu3XAlcBNyRZx+CS1JVV9QTwRJKbgXsYDFrfC6zuq1ZJ0vZSNX1YYO81MTFRk5OT4y5DkvYaSdZU1URrm3dSS5KaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ19RoQSc5J8mCSjUlWNrYfkeT2JGuTrE+yfGjbi5LcnORbSR5I8ro+a5UkPV9vAZFkAXANcC6wDLg4ybJpzS4FNlTVycAZwNVJFnbbfge4o6pOBE4GHuirVknS9vo8gzgV2FhVm6pqK3ATcOG0NgUcliTAocCTwLYkhwNvAP4AoKq2VtXf9lirJGmaPgPiWOCRoeXN3bphq4CTgEeBdcDlVfUc8DJgC/DxJPcmuS7JIa0PSXJJkskkk1u2bJn1TkjS/qrPgEhjXU1bPhu4DzgGOAVY1Z09HAi8Gvi9qnoV8Ayw3RgGQFWtrqqJqppYtGjRLJUuSeozIDYDS4aWFzM4Uxi2HLi1BjYCDwEndvturqqvd+1uZhAYkqQ50mdA3A0cn+S4buD5IuC2aW0eBs4CSHI0cAKwqaoeAx5JckLX7ixgQ4+1SpKmObCvA1fVtiSXAXcCC4Drq2p9khXd9muBq4AbkqxjcEnqyqp6ojvErwA3duGyicHZhiRpjqRq+rDA3mtiYqImJyfHXYYk7TWSrKmqidY276SWJDUZEJKkJgNCktQ00iB1ktOApcPtq+qTPdUkSZoHZgyIJH8IvJzBDW3PdqsLMCAkaR82yhnEBLCs9qWfO0mSZjTKGMQ3gZf0XYgkaX4Z5QziKGBDkm8AP5haWVUX9FaVJGnsRgmID/RdhCRp/tlpQCQ5ALimql45R/VIkuaJnY5BdHMzrE3yD+aoHknSPDHKJaaXAuu7MYhnplY6BiFJ+7ZRAuLf916FJGnemTEgququuShEkjS/jHIn9dP8/VShC4EXAM9U1eF9FiZJGq9RziAOG15O8nPAqX0VJEmaH3b5aa5V9SfAmbNfiiRpPhnlEtNbhhYPYPBsJp/LJEn7uFF+xfTmoffbgG8DF/ZSjSRp3hglIK6rqq8Or0hyOvB4PyVJkuaDUcYgfnfEdZKkfcgOzyCSvA44DViU5FeHNh0OLOi7MEnSeO3sEtNC4NCuzfBPXZ8C3tpnUZKk8dthQHR3UN+V5Iaq+k6SQ6rqmR21lyTtW0YZgzgmyQbgAYAkJyf5L/2WJUkat1EC4qPA2cBfA1TVWuANPdYkSZoHRrqTuqoembbq2R5qkSTNI6PcB/FIktOASrIQeDfd5SZJ0r5rlDOIFcClwLHAZuAU4F/3WJMkaR4Y5WmuTwDvmFpOciSDgPiPPdYlSRqzHZ5BJFmSZHWSzyX5pSQHJ/kI8CDwE3NXoiRpHHZ2BvFJ4C7gFuAc4GvAeuAfV9Vjc1CbJGmMdhYQL66qD3Tv70zyPeA1VfWD/suSJI3bTscguvGGdIuPAQcnOQSgqp7suTZJ0hjtLCCOANbw9wEBcE/3t4CX9VWUJGn8dvYspqVzWIckaZ7Z5TmpJUn7BwNCktTUa0AkOSfJg0k2JlnZ2H5EktuTrE2yPsnyadsXJLk3yef6rFOStL2RAiLJ66f+8U6yKMlxI+yzALgGOBdYBlycZNm0ZpcCG6rqZOAM4OrueU9TLsfnPknSWMwYEEn+HXAl8OvdqhcAfzTCsU8FNlbVpqraCtwEXDitTQGHJQmD2eueBLZ1n7sY+FnguhE+S5I0y0Y5g/h54ALgGYCqepTnT0G6I8cCw48J39ytG7YKOAl4FFgHXF5Vz3XbPgq8D3iOnUhySZLJJJNbtmwZoSxJ0ihGCYitVVUM/m+fqRvlRpDGupq2fDZwH3AMg6fErkpyeJLzgceras1MH1JVq6tqoqomFi1aNGJpkqSZjBIQf5zkY8CLkvwy8KfA74+w32ZgydDyYgZnCsOWA7fWwEbgIeBE4HTggiTfZnBp6swko1zWkiTNkhkDoqo+AtzM4KF9JwD/tqp+d4Rj3w0cn+S4buD5IuC2aW0eBs4CSHJ0d/xNVfXrVbW4u1nvIuB/VNU7R+yTJGkWjDKjHFX1ReCLu3LgqtqW5DLgTmABcH1VrU+yott+LXAVcEOSdQwuSV3ZzT8hSRqzDIYXdtIgeZrtxw6+D0wCv1ZVm3qqbZdNTEzU5OTkuMuQpL1GkjVVNdHaNsoZxG8zGDv4FIP/y78IeAmDiYOuZ3D/giRpHzPKIPU5VfWxqnq6qp6qqtXAeVX1GeDInuuTJI3JKAHxXJJfSHJA9/qFoW07vz4lSdprjRIQ7wB+EXgc+F73/p1JXghc1mNtkqQxmnEMohuEfvMONv/57JYjSZovZgyIJAcBvwS8Ajhoan1VvavHuiRJYzbKJaY/ZPCrpbOBuxjcEf10n0VJksZvlID4yar6TeCZqvoEgyes/qN+y5IkjdsoAfHD7u/fJnklcASwtLeKJEnzwig3yq1OciTwGwyepXQo8Ju9ViVJGrudBkSSA4CnqupvgK8AL5uTqiRJY7fTS0zd5D3e6yBJ+6FRxiC+mOS9SZYkefHUq/fKJEljNcoYxNT9DpcOrSu83CRJ+7RR7qQ+bi4KkSTNLzNeYkpycJLfSLK6Wz6+mzNakrQPG2UM4uPAVuC0bnkz8B96q0iSNC+MEhAvr6oP0d0wV1X/l8HEQZKkfdgoAbG1e7R3ASR5OfCDXquSJI3dKL9i+gBwB7AkyY3A6cC/6LEmSdI8MMqvmL6QZA3wWgaXli6vqid6r0ySNFajzAdxG/Bp4Laqeqb/kiRJ88EoYxBXAz8NbEjyX5O8tZtESJK0DxvlEtNdwF1JFgBnAr8MXA8c3nNtkqQxGmWQmu5XTG8G3ga8GvhEn0VJksZvlDGIzwD/hMEvma4Bvtw95VWStA8b5Qzi48Dbq+pZgCSnJ3l7VV06w36SpL3YKGMQdyQ5JcnFDC4xPQTc2ntlkqSx2mFAJPkp4CLgYuCvgc8AqaqfmaPaJEljtLMziG8B/xN4c1VtBEhyxZxUJUkau53dB/FPgceALyX5/SRn4UP6JGm/scOAqKrPVtXbgBOBLwNXAEcn+b0kb5qj+iRJYzLjndRV9UxV3VhV5wOLgfuAlX0XJkkar1EetfEjVfVkVX2sqs7sqyBJ0vywSwEhSdp/GBCSpCYDQpLUZEBIkpp6DYgk5yR5MMnGJNv98inJEUluT7I2yfoky7v1S5J8KckD3frL+6xTkrS93gKimz/iGuBcYBlwcZJl05pdCmyoqpOBM4CrkywEtgG/VlUnMZjq9NLGvpKkHvV5BnEqsLGqNlXVVuAm4MJpbQo4LEmAQ4EngW1V9d2qugegqp4GHgCO7bFWSdI0fQbEscAjQ8ub2f4f+VXAScCjwDrg8ulzTSRZCrwK+HrrQ5JckmQyyeSWLVtmqXRJUp8B0XpuU01bPpvBndnHAKcAq5L8aCrTJIcCtwDvqaqnWh9SVauraqKqJhYtWjQbdUuS6DcgNgNLhpYXMzhTGLYcuLUGNjKYa+JEgCQvYBAON1aV809I0hzrMyDuBo5Pclw38HwRcNu0Ng8DZwEkORo4AdjUjUn8AfBAVf12jzVKknagt4Coqm3AZcCdDAaZ/7iq1idZkWRF1+wq4LQk64A/A66sqieA04FfBM5Mcl/3Oq+vWiVJ2xtlTurdVlWfBz4/bd21Q+8fBbZ7dHhV/TnOPSFJY+Wd1JKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTb0GRJJzkjyYZGOSlY3tRyS5PcnaJOuTLB91X0lSv3oLiCQLgGuAc4FlwMVJlk1rdimwoapOBs4Ark6ycMR9JUk96vMM4lRgY1VtqqqtwE3AhdPaFHBYkgCHAk8C20bcV5LUoz4D4ljgkaHlzd26YauAk4BHgXXA5VX13Ij7ApDkkiSTSSa3bNkyW7VL0n6vz4BIY11NWz4buA84BjgFWJXk8BH3HaysWl1VE1U1sWjRot2vVpL0PH0GxGZgydDyYgZnCsOWA7fWwEbgIeDEEfeVJPWoz4C4Gzg+yXFJFgIXAbdNa/MwcBZAkqOBE4BNI+4rSerRgX0duKq2JbkMuBNYAFxfVeuTrOi2XwtcBdyQZB2Dy0pXVtUTAK19+6pVkrS9VDUv7e+VJiYmanJyctxlSNJeI8maqppobfNOaklSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1LRP3QeRZAvwnXHXsYuOAp4YdxFzzD7vH+zz3uEfVlXzQXb7VEDsjZJM7ugmlX2Vfd4/2Oe9n5eYJElNBoQkqcmAGL/V4y5gDOzz/sE+7+Ucg5AkNXkGIUlqMiAkSU0GxBxI8uIkX0zyF93fI3fQ7pwkDybZmGRlY/t7k1SSo/qves/saZ+TfDjJt5Lcn+SzSV40Z8XvghG+syT5z932+5O8etR956vd7XOSJUm+lOSBJOuTXD731e+ePfmeu+0Lktyb5HNzV/UsqCpfPb+ADwEru/crgd9qtFkA/CXwMmAhsBZYNrR9CYMZ9r4DHDXuPvXdZ+BNwIHd+99q7T/u10zfWdfmPOC/M5gx8bXA10fddz6+9rDPLwVe3b0/DPjf+3qfh7b/KvAp4HPj7s+uvDyDmBsXAp/o3n8C+LlGm1OBjVW1qaq2Ajd1+035T8D7gL3lVwV71Oeq+kJVbevafQ1Y3G+5u2Wm74xu+ZM18DXgRUleOuK+89Fu97mqvltV9wBU1dPAA8Cxc1n8btqT75kki4GfBa6by6JngwExN46uqu8CdH9/otHmWOCRoeXN3TqSXAD8VVWt7bvQWbRHfZ7mXQz+72y+GaX+HbUZte/zzZ70+UeSLAVeBXx99kucdXva548y+J+753qqrzcHjruAfUWSPwVe0tj0/lEP0VhXSQ7ujvGm3a2tL331edpnvB/YBty4a9XNiRnr30mbUfadj/akz4ONyaHALcB7quqpWaytL7vd5yTnA49X1ZokZ8x2YX0zIGZJVb1xR9uSfG/qFLs77Xy80Wwzg3GGKYuBR4GXA8cBa5NMrb8nyalV9disdWA39NjnqWP8c+B84KzqLuTOMzutf4Y2C0fYdz7akz6T5AUMwuHGqrq1xzpn0570+a3ABUnOAw4CDk/yR1X1zh7rnT3jHgTZH17Ah3n+gO2HGm0OBDYxCIOpgbBXNNp9m71jkHqP+gycA2wAFo27Lzvp44zfGYNrz8ODl9/Yle97vr32sM8BPgl8dNz9mKs+T2tzBnvZIPXYC9gfXsCPA38G/EX398Xd+mOAzw+1O4/BLzv+Enj/Do61twTEHvUZ2Mjgmu593evacfdpB/3crn5gBbCiex/gmm77OmBiV77v+fja3T4Dr2dwaeb+oe/1vHH3p+/veegYe11A+KgNSVKTv2KSJDUZEJKkJgNCktRkQEiSmgwISVKTASF1kvyf7u/SJG+f5WP/m2nL/2s2jy/1wYCQtrcU2KWASLJghibPC4iqOm0Xa5LmnAEhbe+DwE8nuS/JFd2z/D+c5O7uWf//EiDJGd38Bp9icHMUSf4kyZpuvoNLunUfBF7YHe/Gbt3U2Uq6Y38zybokbxs69peT3NzNi3FjumetJPlgkg1dLR+Z8/862m/4LCZpeyuB91bV+QDdP/Tfr6rXJPkx4KtJvtC1PRV4ZVU91C2/q6qeTPJC4O4kt1TVyiSXVdUpjc96C3AKcDJwVLfPV7ptrwJeweCZPl8FTk+yAfh54MSqqvk6kZL2DZ5BSDN7E/DPktzH4PHUPw4c3237xlA4ALw7yVoGc1gsGWq3I68HPl1Vz1bV94C7gNcMHXtzVT3H4LEUS4GngP8HXJfkLcDf7WHfpB0yIKSZBfiVqjqlex1XVVNnEM/8qNHgcc5vBF5XVScD9zJ4gudMx96RHwy9f5bBDHvbGJy13MJgEqY7dqEf0i4xIKTtPc1gSswpdwL/qntUNUl+Kskhjf2OAP6mqv4uyYkMnuo55YdT+0/zFeBt3TjHIuANwDd2VFg3l8IRVfV54D0MLk9JvXAMQtre/cC27lLRDcDvMLi8c083ULyF9hSqdwArktwPPMjgMtOU1cD9Se6pqncMrf8s8DoGj5Au4H1V9VgXMC2HAf8tyUEMzj6u2K0eSiPwaa6SpCYvMUmSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpKb/D5qDjwHuOTV2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cf6da761-05c2-479c-b1f5-60c87849bbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0c9c18a6-ac84-4f7e-b121-399c707b4a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4, 1000)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5335664e-9aca-44b3-a750-576c77dc8f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.84076554]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df48ff-0ebc-40fc-9511-f428fefb3b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0e4e3-3c51-446d-81ce-4708e7c6a284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
