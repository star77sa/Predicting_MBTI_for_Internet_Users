{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a214dfa6-937f-4306-bf82-911785775ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "\n",
    "import abc\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50d15a00-3a86-45be-ad76-27dfc546865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c5a1a6-7e44-4c9f-9620-f84bb5cdb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐글 데이터\n",
    "df = pd.read_csv('G://내 드라이브/Github/Predictiong_MBTI_for_Internet_Users/MBTI 500.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21681be3-3c63-480b-a58e-d1ac8df11067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text', 'type']\n",
    "df = df[['type', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b318a78b-4193-4862-ab37-3c69d37a7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 106067/106067 [00:20<00:00, 5217.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#function to clean the text data\n",
    "def clear_text(data):\n",
    "    data_length=[]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    cleaned_text=[]\n",
    "    for sentence in tqdm(data.text):\n",
    "        sentence=sentence.lower()\n",
    "#         removing links from text data\n",
    "        sentence=re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',sentence)\n",
    "#         removing other symbols\n",
    "        sentence=re.sub('[^0-9a-z]',' ',sentence)\n",
    "        data_length.append(len(sentence.split()))\n",
    "        cleaned_text.append(sentence)\n",
    "    return cleaned_text,data_length\n",
    "\n",
    "df.text, _=clear_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37784595-92c4-4b13-9831-32a18cfc47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'] # features\n",
    "y = df['type']  # labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571aa18a-1488-4409-bdb8-2ac94b64ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelining the vectorizer and the classifier\n",
    "text_clf1 = Pipeline([('tfidf',TfidfVectorizer(sublinear_tf = True)),('clf',LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed469971-ae74-4491-85b2-37899a067135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(sublinear_tf=True)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c9f14f-2fa1-4ee6-a628-5bb62f02a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = text_clf1.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478facc-8eb3-4f1a-8420-6f91e9563105",
   "metadata": {},
   "source": [
    "+++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3174eaad-3f4e-4295-b36f-93f5add7054b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4975d37-07be-4981-bfe4-f123a91075aa",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "889ea8e8-f6b1-46f4-adfd-9abb1ec9dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 3 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 3  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 3  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 1# @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3edd80c1-d8f6-4a5a-a139-8f01a80889dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dcbccc5a-f305-4efc-963b-ba17123dda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()\n",
    "\n",
    "class LinegameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=100, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 0.1\n",
    "        self._time = 1\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 0.1\n",
    "        self._time = 1\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            self.reset()\n",
    "        elif self._time==1: \n",
    "            print(\"### new episode start! ###\")\n",
    "            print(\" \")\n",
    "        else: \n",
    "            print(\" \")\n",
    "        \n",
    "        print(\"{}-1. current state = {}\".format(self._time,self._state))\n",
    "        \n",
    "        # Pipelining the vectorizer and the classifier\n",
    "        text_clf1 = Pipeline([('tfidf',TfidfVectorizer(sublinear_tf = True)),('clf',LinearSVC(C= self._state))])\n",
    "        text_clf1.fit(X_train, y_train)\n",
    "        \n",
    "        reward = metrics.accuracy_score(y_val, text_clf1.predict(X_val))\n",
    "        \n",
    "        #self._state += (action-500)/500\n",
    "        #print(\"{}-2. action = {}\".format(self._time,(action-500)/500))\n",
    "        self._state += (action-50)/500\n",
    "        print(\"{}-2. action = {}\".format(self._time,(action-50)/500))\n",
    "        \n",
    "        \n",
    "        if self._state < 0.1:\n",
    "          print(\"    **exit(current state<0.1) => reset current state as 0.**\")\n",
    "          self._state = 0.1\n",
    "        if self._state > 1.0 : \n",
    "          print(\"   **exit(current state >1.0) => reset current state as 1.**\")\n",
    "          self._state = 1.0\n",
    "        \n",
    "        print(\"{}-3. reward = {}\".format(self._time,reward))\n",
    "        returns.append(reward) ############################# !!!\n",
    "        \n",
    "        print(\"{}-4. future state = {}\".format(self._time,self._state))\n",
    "        \n",
    "        self._time += 1\n",
    "        if self._time > 20: self._episode_ended = True\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            print(\" \")\n",
    "            print(\"### episode ended ###\")\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "97396a5b-a99a-4c65-a426-62155444a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### new episode start! ###\n",
      " \n",
      "1-1. current state = 0.1\n",
      "1-2. action = 0.062\n",
      "1-3. reward = 0.8407655321957198\n",
      "1-4. future state = 0.162\n",
      " \n",
      "2-1. current state = 0.162\n",
      "2-2. action = -0.058\n",
      "2-3. reward = 0.847364947676063\n",
      "2-4. future state = 0.10400000000000001\n",
      " \n",
      "3-1. current state = 0.10400000000000001\n",
      "3-2. action = -0.064\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "3-3. reward = 0.8414254737437541\n",
      "3-4. future state = 0.1\n",
      " \n",
      "4-1. current state = 0.1\n",
      "4-2. action = 0.048\n",
      "4-3. reward = 0.8407655321957198\n",
      "4-4. future state = 0.14800000000000002\n",
      " \n",
      "5-1. current state = 0.14800000000000002\n",
      "5-2. action = 0.078\n",
      "5-3. reward = 0.8468935608560385\n",
      "5-4. future state = 0.22600000000000003\n",
      " \n",
      "6-1. current state = 0.22600000000000003\n",
      "6-2. action = -0.076\n",
      "6-3. reward = 0.848307721316112\n",
      "6-4. future state = 0.15000000000000002\n",
      " \n",
      "7-1. current state = 0.15000000000000002\n",
      "7-2. action = -0.004\n",
      "7-3. reward = 0.8470821155840482\n",
      "7-4. future state = 0.14600000000000002\n",
      " \n",
      "8-1. current state = 0.14600000000000002\n",
      "8-2. action = -0.016\n",
      "8-3. reward = 0.8464221740360139\n",
      "8-4. future state = 0.13\n",
      " \n",
      "9-1. current state = 0.13\n",
      "9-2. action = -0.036\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "9-3. reward = 0.8446309041199208\n",
      "9-4. future state = 0.1\n",
      " \n",
      "10-1. current state = 0.1\n",
      "10-2. action = -0.062\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "10-3. reward = 0.8407655321957198\n",
      "10-4. future state = 0.1\n",
      " \n",
      "11-1. current state = 0.1\n",
      "11-2. action = 0.016\n",
      "11-3. reward = 0.8407655321957198\n",
      "11-4. future state = 0.116\n",
      " \n",
      "12-1. current state = 0.116\n",
      "12-2. action = -0.018\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "12-3. reward = 0.8426510794758179\n",
      "12-4. future state = 0.1\n",
      " \n",
      "13-1. current state = 0.1\n",
      "13-2. action = -0.054\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "13-3. reward = 0.8407655321957198\n",
      "13-4. future state = 0.1\n",
      " \n",
      "14-1. current state = 0.1\n",
      "14-2. action = -0.04\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "14-3. reward = 0.8407655321957198\n",
      "14-4. future state = 0.1\n",
      " \n",
      "15-1. current state = 0.1\n",
      "15-2. action = -0.01\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "15-3. reward = 0.8407655321957198\n",
      "15-4. future state = 0.1\n",
      " \n",
      "16-1. current state = 0.1\n",
      "16-2. action = 0.1\n",
      "16-3. reward = 0.8407655321957198\n",
      "16-4. future state = 0.2\n",
      " \n",
      "17-1. current state = 0.2\n",
      "17-2. action = -0.088\n",
      "17-3. reward = 0.8481191665881022\n",
      "17-4. future state = 0.11200000000000002\n",
      " \n",
      "18-1. current state = 0.11200000000000002\n",
      "18-2. action = 0.058\n",
      "18-3. reward = 0.8420854152917885\n",
      "18-4. future state = 0.17\n",
      " \n",
      "19-1. current state = 0.17\n",
      "19-2. action = 0.076\n",
      "19-3. reward = 0.8474592250400679\n",
      "19-4. future state = 0.246\n",
      " \n",
      "20-1. current state = 0.246\n",
      "20-2. action = -0.054\n",
      "20-3. reward = 0.8495333270481757\n",
      "20-4. future state = 0.192\n",
      " \n",
      "### episode ended ###\n"
     ]
    }
   ],
   "source": [
    "environment = LinegameEnv()\n",
    "utils.validate_py_environment(environment, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "30db82bc-9f86-4477-b22c-f8fca0e0004f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8407655321957198,\n",
       " 0.847364947676063,\n",
       " 0.8414254737437541,\n",
       " 0.8407655321957198,\n",
       " 0.8468935608560385,\n",
       " 0.848307721316112,\n",
       " 0.8470821155840482,\n",
       " 0.8464221740360139,\n",
       " 0.8446309041199208,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8426510794758179,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8407655321957198,\n",
       " 0.8481191665881022,\n",
       " 0.8420854152917885,\n",
       " 0.8474592250400679,\n",
       " 0.8495333270481757]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9abe44bf-55d8-4d35-80cf-19b30f31d1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#iterations = range(0, 2*7 + 1, 1)\n",
    "iterations = range(0,20)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Return')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "29481664-2f3c-4dc7-ba62-0406743ca5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = LinegameEnv\n",
    "eval_py_env = LinegameEnv\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe0b61-a405-4327-a9b5-a77d770e26fc",
   "metadata": {},
   "source": [
    "### 에이전트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c6f70fba-e29f-4f39-b920-b3cb4f62602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step_counter = tf.Variable(0)\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd3c2c-ff20-4fec-b48f-b6bd02efbd71",
   "metadata": {},
   "source": [
    "### Policies\n",
    "\n",
    "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
    "\n",
    "In this tutorial:\n",
    "\n",
    "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
    "-   The policy returns an action (left or right) for each `time_step` observation.\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6e90c60e-0d1e-41b2-b0c6-2a45033e2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed967874-eb82-4041-a137-46170d890790",
   "metadata": {},
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a679b831-73c5-4965-b435-449d05e7104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669e6ca-e991-436e-85fb-c8b5f5697de7",
   "metadata": {},
   "source": [
    "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439c44c-e78c-46f5-914d-d1be049cef21",
   "metadata": {},
   "source": [
    "### Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n",
    "\n",
    "정책을 평가하는 데 사용되는 가장 일반적인 메트릭은 평균 수익률입니다. 수익률은 한 에피소드에 대한 환경에서 정책을 실행하는 동안 얻은 보상의 합계입니다. 몇 개의 에피소드가 실행되어 평균 수익을 창출합니다. 다음 함수는 정책, 환경 및 에피소드 수에 따라 정책의 평균 수익률을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3dbcc350-de50-4d9b-a41b-6f1f35d053d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "953834ae-d8ab-46a8-84c9-29fe7cdea7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fcb6ac89-a4e6-45b9-b7d1-982cb4e0eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            print(f\"{time_step.reward} 현재의 리워드다다다다다\")\n",
    "        total_return += episode_return\n",
    "        print(f\"{episode_return}이다다다ㅏ다다\")\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0d5ce-9d0b-45fb-9069-8b65b0a04d4d",
   "metadata": {},
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "894a8815-c7c6-4519-9a6a-b99784026327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### new episode start! ###\n",
      " \n",
      "1-1. current state = 0.1\n",
      "1-2. action = 0.09\n",
      "1-3. reward = 0.8407655321957198\n",
      "1-4. future state = 0.19\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "2-1. current state = 0.19\n",
      "2-2. action = 0.092\n",
      "2-3. reward = 0.848307721316112\n",
      "2-4. future state = 0.28200000000000003\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "3-1. current state = 0.28200000000000003\n",
      "3-2. action = -0.094\n",
      "3-3. reward = 0.8494390496841708\n",
      "3-4. future state = 0.18800000000000003\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "4-1. current state = 0.18800000000000003\n",
      "4-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "4-3. reward = 0.848307721316112\n",
      "4-4. future state = 0.1\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "5-1. current state = 0.1\n",
      "5-2. action = 0.038\n",
      "5-3. reward = 0.8407655321957198\n",
      "5-4. future state = 0.138\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "6-1. current state = 0.138\n",
      "6-2. action = -0.074\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "6-3. reward = 0.84538512303196\n",
      "6-4. future state = 0.1\n",
      "[0.] 현재의 리워드다다다다다\n",
      " \n",
      "7-1. current state = 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-3b7f8579967a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_avg_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-153-74d4291e3b08>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[1;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mepisode_return\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{time_step.reward} 현재의 리워드다다다다다\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;34m'but saw action with shape %s:\\n   %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 (self.batch_size, action.shape, action))\n\u001b[1;32m--> 315\u001b[1;33m       outputs = tf.numpy_function(\n\u001b[0m\u001b[0;32m    316\u001b[0m           \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m           \u001b[0mflat_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mnumpy_function\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[0mSingle\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcomputes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m   \"\"\"\n\u001b[1;32m--> 770\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mpy_func_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mpy_func_common\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    634\u001b[0m   \"\"\"\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_isolated_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_step_py\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    296\u001b[0m         packed = tf.nest.pack_sequence_as(\n\u001b[0;32m    297\u001b[0m             structure=self.action_spec(), flat_sequence=flattened_actions)\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\batched_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_envs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m       \u001b[0mtime_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-fdda9641a9fe>\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Pipelining the vectorizer and the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtext_clf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublinear_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0m\u001b[0;32m    976\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19265a-86e6-4792-a73b-8530626e1dff",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n",
    "\n",
    "재생 버퍼는 환경에서 수집된 데이터를 추적합니다. 이 자습서에서는 tf_agents.replay_buffers.tf_uniform_replay_buffer를 사용합니다.가장 일반적인 TFUniformReplayBuffer입니다. 생성자는 수집할 데이터에 대한 사양을 요구합니다. 이 기능은 collect_data_spec 메서드를 사용하여 에이전트에서 사용할 수 있습니다. 배치 크기와 최대 버퍼 길이도 필요합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d288b12-690d-4efa-9e69-bcb5226521a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd3640-3827-4b78-9cd0-14e4fc92c372",
   "metadata": {},
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab33d472-3a88-4402-9158-6e2b363c7751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(100)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array(0), maximum=array(2147483647)),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b328aeab-9cc8-43ac-afda-dc7b0eb3caff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fab0a-1d89-4d2a-884e-8086b4e1f025",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a8340a15-7519-4b39-83c2-779556cc602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "4-1. current state = 0.1\n",
      "4-2. action = -0.058\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "4-3. reward = 0.8407655321957198\n",
      "4-4. future state = 0.1\n"
     ]
    }
   ],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=1)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcfb50-6e21-4e2b-a1fa-8c92adebdbab",
   "metadata": {},
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9582980c-f8b1-4adc-8151-e1854bf9ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the curious:\n",
    "# Uncomment to peel one of these off and inspect it.\n",
    "# iter(replay_buffer.as_dataset()).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b88f79-45a4-4620-9b84-5e3110562a05",
   "metadata": {},
   "source": [
    "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "This dataset is also optimized by running parallel calls and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "29a6ba62-edaa-4414-a08f-ec7aef72e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(Trajectory(\n",
       "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(64, 2, 1), dtype=tf.int32, name=None),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), BufferInfo(ids=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), probabilities=TensorSpec(shape=(64,), dtype=tf.float32, name=None)))>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "817fe5f9-5f0f-4009-a854-2cf12987ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026D2B103340>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "72e14f16-d4b6-4ad5-96c7-fcbb1b96374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the curious:\n",
    "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
    "# Compare this representation of replay data \n",
    "# to the collection of individual trajectories shown earlier.\n",
    "\n",
    "# iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3f375-becf-4212-82ea-367dc8085c02",
   "metadata": {},
   "source": [
    "### Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa52b1d4-c6ef-4192-946d-b4b45cf34028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### new episode start! ###\n",
      " \n",
      "1-1. current state = 0.1\n",
      "1-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "1-3. reward = 0.8407655321957198\n",
      "1-4. future state = 0.1\n",
      " \n",
      "2-1. current state = 0.1\n",
      "2-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "2-3. reward = 0.8407655321957198\n",
      "2-4. future state = 0.1\n",
      " \n",
      "3-1. current state = 0.1\n",
      "3-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "3-3. reward = 0.8407655321957198\n",
      "3-4. future state = 0.1\n",
      " \n",
      "4-1. current state = 0.1\n",
      "4-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "4-3. reward = 0.8407655321957198\n",
      "4-4. future state = 0.1\n",
      " \n",
      "5-1. current state = 0.1\n",
      "5-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "5-3. reward = 0.8407655321957198\n",
      "5-4. future state = 0.1\n",
      " \n",
      "6-1. current state = 0.1\n",
      "6-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "6-3. reward = 0.8407655321957198\n",
      "6-4. future state = 0.1\n",
      " \n",
      "7-1. current state = 0.1\n",
      "7-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "7-3. reward = 0.8407655321957198\n",
      "7-4. future state = 0.1\n",
      " \n",
      "### episode ended ###\n",
      " \n",
      "5-1. current state = 0.1\n",
      "5-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "5-3. reward = 0.8407655321957198\n",
      "5-4. future state = 0.1\n",
      " \n",
      "6-1. current state = 0.1\n",
      "6-2. action = -0.026\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "6-3. reward = 0.8407655321957198\n",
      "6-4. future state = 0.1\n",
      " \n",
      "7-1. current state = 0.1\n",
      "7-2. action = -0.1\n",
      "    **exit(current state<0.1) => reset current state as 0.**\n",
      "7-3. reward = 0.8407655321957198\n",
      "7-4. future state = 0.1\n",
      " \n",
      "### episode ended ###\n",
      "WARNING:tensorflow:5 out of the last 28 calls to <tensorflow.python.eager.def_function.Function object at 0x0000026D3A4F45E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 28 calls to <tensorflow.python.eager.def_function.Function object at 0x0000026D3A4F45E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1. current state = 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-462ee3ad05c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollect_steps_per_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mcollect_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-144-64fcfe51e8ac>\u001b[0m in \u001b[0;36mcollect_step\u001b[1;34m(environment, policy, buffer)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnext_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtraj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;34m'but saw action with shape %s:\\n   %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 (self.batch_size, action.shape, action))\n\u001b[1;32m--> 315\u001b[1;33m       outputs = tf.numpy_function(\n\u001b[0m\u001b[0;32m    316\u001b[0m           \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m           \u001b[0mflat_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mnumpy_function\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[0mSingle\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcomputes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m   \"\"\"\n\u001b[1;32m--> 770\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mpy_func_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36mpy_func_common\u001b[1;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[0;32m    634\u001b[0m   \"\"\"\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_isolated_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_isolated_step_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_step_py\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_step_py\u001b[1;34m(*flattened_actions)\u001b[0m\n\u001b[0;32m    296\u001b[0m         packed = tf.nest.pack_sequence_as(\n\u001b[0;32m    297\u001b[0m             structure=self.action_spec(), flat_sequence=flattened_actions)\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\batched_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_envs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m       \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m       \u001b[0mtime_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_agents\\environments\\py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-fdda9641a9fe>\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Pipelining the vectorizer and the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtext_clf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublinear_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m                     \u001b[1;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96669f69-ca6e-4528-a274-3e6fe36aa939",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a3f1f-5c8d-4e34-ac1b-778e3d40cadb",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
    "\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ad1d7b56-4462-4a6f-808e-e4c60664b92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cf6da761-05c2-479c-b1f5-60c87849bbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0c9c18a6-ac84-4f7e-b121-399c707b4a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4, 1000)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5335664e-9aca-44b3-a750-576c77dc8f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.84076554]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df48ff-0ebc-40fc-9511-f428fefb3b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0e4e3-3c51-446d-81ce-4708e7c6a284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
